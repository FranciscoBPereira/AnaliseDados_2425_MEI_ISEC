{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Inference**\n"
      ],
      "metadata": {
        "id": "kwhOjcLhQFU-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmGqbsINsIlf"
      },
      "outputs": [],
      "source": [
        "#Install Transformers library to run this notebook.\n",
        "\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "BsBsLZbchzLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1 - An Introduction to Inference Pipelines**"
      ],
      "metadata": {
        "id": "TMMrWPK-USsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The call to pipeline() specifies the task and the model\n",
        "\n",
        "# The task specification is mandatory. In this case, we are creating a pipeline for sentiment analysis\n",
        "# Model specification is optional. By default, the pipeline selects a particular pretrained model\n",
        "# that has been fine-tuned for sentiment analysis in English: DistilBERT base uncased finetuned SST-2\n",
        "# https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english\n",
        "\n",
        "\n",
        "MSA1 = pipeline(\"sentiment-analysis\")\n",
        "MSA1(\"I've been waiting for a HuggingFace course my whole life.\")"
      ],
      "metadata": {
        "id": "7bxrhYHFhfit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try  model MSA1 with several other sentences\n",
        "\n",
        "str1 = ['I hate this so much!', 'Your support team is useless',\n",
        "       'Disliking watercraft is not really my thing.', 'I would really truly love going out in this weather!',\n",
        "       'You should see their decadent dessert menu.',  'I love my mobile but would not recommend it to any of my colleagues.']\n",
        "\n",
        "MSA1(str1)"
      ],
      "metadata": {
        "id": "22o4bEqJhwze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't forget that language models can be biased and unfair.\n",
        "# Most of the bias comes from training data\n",
        "\n",
        "str2 = ['I am from Portugal.', 'I am from India.', 'I am from Iraq.']\n",
        "\n",
        "MSA1(str2)"
      ],
      "metadata": {
        "id": "SmZYC-3gM1QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try the pipeline with other sentences\n",
        "\n",
        "str3 = []\n",
        "\n",
        "# Complete the inference call\n",
        "\n"
      ],
      "metadata": {
        "id": "EJpcX1_oMwg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can specify another model as a parameter:\n",
        "\n",
        "# The bertweet-sentiment-analysis model was fine tuned for sentiment analysis (the base model is BERTweet, a RoBERTa model trained on English tweets)\n",
        "# https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis\n",
        "\n",
        "# Just add an additional parameter (model) to the pipeline function\n",
        "\n",
        "MSA2 = pipeline(\"sentiment-analysis\", model = 'finiteautomata/bertweet-base-sentiment-analysis')\n"
      ],
      "metadata": {
        "id": "Q4qPYC90llJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRY\n",
        "\n",
        "# Apply the new model on the previous sentences (str1, str2 and str3) and compare performance\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iKnqH8HjnZKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Do you notice any difference in the sentiment analysis performed by this model?"
      ],
      "metadata": {
        "id": "4SpQhLtaOH2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero-shot classification task: https://huggingface.co/tasks/zero-shot-classification\n",
        "\n",
        "ZS1 = pipeline('zero-shot-classification')\n",
        "\n",
        "ZS1('This is a course about the Transformers library', candidate_labels=['education', 'politics', 'business'])"
      ],
      "metadata": {
        "id": "TzvHhc_cp-jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quiz:\n",
        "\n",
        "Which model was selected by default to the Zero-shot classification task?\n",
        "\n",
        "What is the address of the model card?"
      ],
      "metadata": {
        "id": "pMoLqj6gqsPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # TRY\n",
        "\n",
        " # Apply pipeline ZS1 to other sentences / candidate labels\n",
        "\n",
        "strC = ['I am travelling to Italy', 'I am listening to music.', 'This is the best pizza.']\n",
        "\n",
        "candidate_labels=['education', 'holidays', 'business', 'travel', 'cooking']\n",
        "\n",
        "ZS1(strC, candidate_labels)\n"
      ],
      "metadata": {
        "id": "4vE3h6Rdq3jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # TRY\n",
        "\n",
        " # 2. Select another model for this task, create pipeline ZS2 and compare performance\n",
        "\n"
      ],
      "metadata": {
        "id": "su0LQJn3oHb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Generation task: https://huggingface.co/tasks/text-generation\n",
        "\n",
        "Gen1 = pipeline('text-generation')\n",
        "\n",
        "Gen1('In this course, we will teach you how to',  max_length=100)\n"
      ],
      "metadata": {
        "id": "H2rTFOc7sOFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRY\n",
        "\n",
        " # 1. Apply pipeline Gen1 to generate other sentences\n",
        "\n",
        " # 2. Select another model for this task, create pipeline Gen2 and compare outputs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e2TUhEeKumVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Translation task: https://huggingface.co/tasks/translation\n",
        "\n",
        "# In this task, we explicitly specify the model and address the problem of translating French to English\n",
        "# https://huggingface.co/Helsinki-NLP/opus-mt-en-fr\n",
        "\n",
        "\n",
        "T1 = pipeline('translation', model='Helsinki-NLP/opus-mt-fr-en')\n",
        "\n",
        "T1(['Ce cours est produit par Hugging Face.', 'Bonne nuit.', 'Le Portugal bat la France en finale de l''Euro 2016.'])\n"
      ],
      "metadata": {
        "id": "i4Mg59YBMHts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There are models that can handle several languages. This one can translate from many different languages to English\n",
        "# https://huggingface.co/Helsinki-NLP/opus-mt-mul-en\n",
        "\n",
        "\n",
        "T2 = pipeline('translation', model='Helsinki-NLP/opus-mt-mul-en')\n",
        "\n",
        "T2(['Olá.', 'Boa noite.', 'A capital de Portugal é Lisboa'])\n",
        "\n"
      ],
      "metadata": {
        "id": "iJXzTLdxQD1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T2(['Hola.', 'Buenas noches.', 'Hoy no llueve.'])\n"
      ],
      "metadata": {
        "id": "zmkx6PLJSzzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2 - A Detailed View on Pipeline Operations**"
      ],
      "metadata": {
        "id": "auZ2mD_6ZKPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A. Preprocessing with a Tokenizer**"
      ],
      "metadata": {
        "id": "m8rTEW7HR0Hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizers transform raw text input into tokens and then numerical values\n",
        "# Two tokenizers are selected with AutoClasses - they guess which tokenizer to download, given the checkpoint name of the model\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "my_tok1 = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "my_tok2 = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
        "\n",
        "sequence = ['Using a Transformer is simple. Dont you agree?', 'Are you feeling better?']\n",
        "\n"
      ],
      "metadata": {
        "id": "g4C1Gr0xrsWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete tokenization with my_tok1\n",
        "\n",
        "r1 = my_tok1(sequence, padding=True, return_tensors=\"tf\")\n",
        "\n",
        "print(r1.input_ids)\n",
        "print(r1.attention_mask)\n"
      ],
      "metadata": {
        "id": "6oj85Nbgr1GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete tokenization with my_tok2\n",
        "\n",
        "r2 = my_tok2(sequence, padding=True, return_tensors=\"tf\")\n",
        "\n",
        "print(r2.input_ids)\n",
        "print(r2.attention_mask)"
      ],
      "metadata": {
        "id": "ytQQzhSbxFuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization proceeds in two steps: split into tokens and map to integers\n",
        "\n",
        "# Step 1:\n",
        "\n",
        "tokens1 = my_tok1.tokenize(sequence)\n",
        "\n",
        "print('Tok1: ', tokens1)\n",
        "\n",
        "tokens2 = my_tok2.tokenize(sequence)\n",
        "\n",
        "print('Tok2: ', tokens2)\n"
      ],
      "metadata": {
        "id": "e6Gd9fHcvpYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2:\n",
        "\n",
        "ids1 = my_tok1.convert_tokens_to_ids(tokens1)\n",
        "\n",
        "print('Tok 1: ', ids1)\n",
        "\n",
        "ids2 = my_tok2.convert_tokens_to_ids(tokens2)\n",
        "\n",
        "print('Tok 2: ', ids2)\n"
      ],
      "metadata": {
        "id": "qw-l5EBWwDal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_string = my_tok1.decode([7993, 170, 13809, 23763, 1110, 3014, 119, 1790, 1204, 1128, 5340, 136, 2372, 1128, 2296, 1618, 136])\n",
        "\n",
        "print(decoded_string)"
      ],
      "metadata": {
        "id": "bMij6aXawW6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B. Inference with a Model**"
      ],
      "metadata": {
        "id": "XvFJ4CypT_DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detailed documentation about the classes used in the next sections can be found here\n",
        "# https://huggingface.co/transformers/v3.0.2/model_doc/auto.html\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import TFAutoModel\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "# Checkpoint name of the selected model: https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n"
      ],
      "metadata": {
        "id": "IM1GP_JzUK_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the tokenizer from the model\n",
        "# https://huggingface.co/transformers/v3.0.2/main_classes/tokenizer.html\n",
        "# https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "31RiMW2do3MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Tokenization to raw inputs\n",
        "\n",
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "rSS3ifjCYQfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain the parameters specified and the output of tokenization"
      ],
      "metadata": {
        "id": "Pi6ves4zsrmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the model\n",
        "# https://huggingface.co/transformers/v3.0.2/model_doc/auto.html\n",
        "\n",
        "# Two models are created. The first is the base Transformer module and the second is the full Transformer for sequence classification\n",
        "\n",
        "modelH = TFAutoModel.from_pretrained(checkpoint)\n",
        "\n",
        "modelFinal = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "w1Uk0A-UYVE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the output of modelH\n",
        "\n",
        "outH = modelH(inputs)\n",
        "\n",
        "print(outH)\n",
        "\n",
        "print('Shape: ', outH.last_hidden_state.shape)"
      ],
      "metadata": {
        "id": "4qqTYiOpbdEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the outputs of modelFinal\n",
        "\n",
        "outFinal = modelFinal(inputs)\n",
        "\n",
        "print(outFinal)\n",
        "\n",
        "print('Shape: ', outFinal.logits.shape)"
      ],
      "metadata": {
        "id": "GJ3qoGuyb3TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain the shape of the outputs provided by the models"
      ],
      "metadata": {
        "id": "kxbRSR3nvYx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**C. Postprocessing the Output**"
      ],
      "metadata": {
        "id": "SJXasSyCcrt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The classification transformer outputs raw scores. They can be normalized (converted to probabilities)\n",
        "# by passing them through a softmax layer\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "predictions = tf.math.softmax(outFinal.logits, axis=-1)\n",
        "\n",
        "print(np.argmax(predictions.numpy(), axis=1))\n",
        "\n",
        "print('LABELS: ', modelFinal.config.id2label)\n"
      ],
      "metadata": {
        "id": "mC1xxwz7czEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.3 - Models**"
      ],
      "metadata": {
        "id": "nSHX615gJLmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import a BERT model for a TensorFlow environment\n",
        "\n",
        "# https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#\n",
        "# https://huggingface.co/docs/transformers/model_doc/bert\n",
        "\n",
        "from transformers import TFBertModel\n",
        "\n",
        "modelB = TFBertModel.from_pretrained('bert-base-cased')\n"
      ],
      "metadata": {
        "id": "fdpb8bqqJPzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the model configuration details\n",
        "\n",
        "modelB.config"
      ],
      "metadata": {
        "id": "vGCqPAFlJrJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use this BERT model for sequence classification: https://huggingface.co/tasks/text-classification\n",
        "\n",
        "sequences = ['This dog is cute.', 'I hate you.']\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-WluAGZeJ0eE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize sentences for BERT\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "tokenizerB = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "encoded = tokenizerB(sequences, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "print(encoded)"
      ],
      "metadata": {
        "id": "RGMkdPYFqc_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the model to the encoded sentences and obtain results\n",
        "\n",
        "outA = modelB(encoded)\n",
        "\n",
        "print(outA)\n",
        "\n",
        "print('Shape: ', outA.last_hidden_state.shape)"
      ],
      "metadata": {
        "id": "uGUwKy2GLnF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the previous sections corresponds to the vector delivered by the final hidden state.\n",
        "\n",
        "Perform the required changes, in order to obtain a final outcome for the text classification task. You can either:\n",
        "1. Add a post processing unit to the model\n",
        "\n",
        "2. Select another BERT model that already contains a classification head for your task (https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#)"
      ],
      "metadata": {
        "id": "spwuldLs2apH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the missing code\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5G-6IlwyL-SU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}