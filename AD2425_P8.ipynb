{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FranciscoBPereira/AnaliseDados_2425_MEI_ISEC/blob/main/AD2425_P8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1CXm_kbADnl"
      },
      "outputs": [],
      "source": [
        "# Setup, Version check and Common imports\n",
        "\n",
        "# Python ≥3.8 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)\n",
        "\n",
        "print('Python version: ', sys.version_info)\n",
        "print('TF version: ', tf.__version__)\n",
        "print('Keras version: ', keras.__version__)\n",
        "print('GPU is', 'available' if tf.config.list_physical_devices('GPU') else 'NOT AVAILABLE')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download IMDB dataset from Keras: https://keras.io/api/datasets/imdb/\n",
        "# Reviews are already preprocessed and ready to use\n",
        "\n",
        "# The raw dataset is available here: https://ai.stanford.edu/~amaas/data/sentiment/\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Check the documentation of the load_data function\n",
        "\n",
        "max_features = 10000\n",
        "common_words = 10\n",
        "\n",
        "# Load train and test datasets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=max_features, skip_top=common_words)\n",
        "\n",
        "# Load a dictionary that will be used to decode reviews\n",
        "word_index = keras.datasets.imdb.get_word_index()"
      ],
      "metadata": {
        "id": "WoaHH6O_AXZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz 1**\n",
        "\n",
        "Consult the documentation to answer the following questions:\n",
        "\n",
        "1. What preprocessing operations have been done to the original reviews?\n",
        "\n",
        "2. The load_data() method has 2 parameters. Why are they important to prepare the dataset?\n"
      ],
      "metadata": {
        "id": "nt18YjU3IsSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1. Consult Some Reviews**"
      ],
      "metadata": {
        "id": "oQ2fUhG7FKSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing some reviews\n",
        "\n",
        "# By relying on the retrieved dictionary, it is also possible to visualize the decoded review\n",
        "\n",
        "# Labels: 0(Bad), 1(Good)\n",
        "\n",
        "# Choose review\n",
        "review = 0\n",
        "\n",
        "print(\"Word count: \" ,len(x_train[review]))\n",
        "print(x_train[review])\n",
        "\n",
        "tam = len(x_train[review])\n",
        "print('Label ', y_train[review])\n",
        "\n",
        "\n",
        "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
        "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "    id_to_word[id_] = token\n",
        "\" \".join([id_to_word[id_] for id_ in x_train[review][:tam]])"
      ],
      "metadata": {
        "id": "a5HZpVTWKnCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2. Trim Reviews**"
      ],
      "metadata": {
        "id": "byQbzgm2FOdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trim reviews and keep just the last maxlen words\n",
        "\n",
        "# The classifier will perform sentiment anaslysis just considering these last words of the reviews\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
        "\n",
        "maxlen = 20\n",
        "\n",
        "x_trainP = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_testP = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0XM17NCON77k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing reviews after trimming\n",
        "\n",
        "# Escolher a review\n",
        "review = 10\n",
        "\n",
        "print(\"Word count: \" ,len(x_trainP[review]))\n",
        "\n",
        "print('Label ', y_train[review])\n",
        "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
        "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "    id_to_word[id_] = token\n",
        "\" \".join([id_to_word[id_] for id_ in x_trainP[review][:tam]])\n"
      ],
      "metadata": {
        "id": "mQgUcrE7tx_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Classifiers for Sentiment Analysis**"
      ],
      "metadata": {
        "id": "Q41a3wV_FkZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1 Model A - Multilayer Perceptron (MLP)**"
      ],
      "metadata": {
        "id": "DXWweSxfFrLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Baseline Feed-Forward Neural Network\n",
        "\n",
        "# Complete the model with two hidden layers, each with 20 nodes (default parameters)\n",
        "# The last layer should have 1 node with sigmoid activation\n",
        "\n",
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "inputs = keras.Input(shape=[maxlen, 1])\n",
        "x = keras.layers.Flatten()(inputs)\n",
        "\n",
        "### Complete the Missing layers ###\n",
        "\n",
        "output = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "modelA = keras.Model(inputs, output)"
      ],
      "metadata": {
        "id": "0Q6eeCJhuDDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "modelA.summary()"
      ],
      "metadata": {
        "id": "zSn2V2mKI_t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelA.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = modelA.fit(x_trainP, y_train, epochs=10, validation_split=0.2)"
      ],
      "metadata": {
        "id": "ZH10wKFVJf4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate ModelA on the test set\n",
        "\n",
        "modelA.evaluate(x_testP, y_test)"
      ],
      "metadata": {
        "id": "-tLmPch8JvzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz:**\n",
        "How do you evaluate the performance of Model A?"
      ],
      "metadata": {
        "id": "QbQaL8TvGikL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2. MultiLayer Perceptron with Word Embedding**"
      ],
      "metadata": {
        "id": "mj10Y3z8Gvi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Add a trainable embedding layer. The embedding should habe 8 dimensions\n",
        "# The remaining layers should be identical to Model A\n",
        "\n",
        "# https://www.tensorflow.org/tutorials/text/word_embeddings\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
        "\n",
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "output_emb = 8\n",
        "\n",
        "inputs = keras.Input(shape=[maxlen])\n",
        "emb = keras.layers.Embedding(max_features,  output_emb)(inputs)\n",
        "x = keras.layers.Flatten()(emb)\n",
        "\n",
        "### Complete the Missing layers ###\n",
        "\n",
        "output = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "modelB = keras.Model(inputs, output)\n"
      ],
      "metadata": {
        "id": "tnGFsix7KBVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelB.summary()"
      ],
      "metadata": {
        "id": "zxXUPt85KM8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelB.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = modelB.fit(x_trainP, y_train,\n",
        "                      epochs=10,\n",
        "                      validation_split=0.2)"
      ],
      "metadata": {
        "id": "CoKQK-TXKXC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model B on the test set\n",
        "\n",
        "modelB.evaluate(x_testP, y_test)"
      ],
      "metadata": {
        "id": "sntWGrYdKeX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz:**\n",
        "How do you evaluate the performance of Model B?"
      ],
      "metadata": {
        "id": "ntDKPNUAHLbl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3. Recurrent Neural Network (RNN) with Word Embedding**"
      ],
      "metadata": {
        "id": "Wvu6cMpcHW7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# The feed-forward cells from hidden layers are replaced by recurrent GRU cells\n",
        "# Everything else is identical to ModelB\n",
        "\n",
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "output_emb = 8\n",
        "\n",
        "inputs = keras.Input(shape=[maxlen])\n",
        "emb = keras.layers.Embedding(max_features,  output_emb)(inputs)\n",
        "\n",
        "x = keras.layers.GRU(20, return_sequences=True)(emb)\n",
        "x = keras.layers.GRU(20, return_sequences=False)(x)\n",
        "\n",
        "output = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "modelC = keras.Model(inputs, output)"
      ],
      "metadata": {
        "id": "Ugom5K-UKjZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelC.summary()"
      ],
      "metadata": {
        "id": "32gpfLwhLEag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelC.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = modelC.fit(x_trainP, y_train,\n",
        "                      epochs=10,\n",
        "                      validation_split=0.2)"
      ],
      "metadata": {
        "id": "ypUMdFhvLIHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model C on the test set\n",
        "\n",
        "modelC.evaluate(x_testP, y_test)"
      ],
      "metadata": {
        "id": "Fj24qXP-Lb2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz:**\n",
        "How do you evaluate the performance of Model C?"
      ],
      "metadata": {
        "id": "4qKIjGTuH-dA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4. Recurrent Neural Network with Pretrained Embedding**"
      ],
      "metadata": {
        "id": "SWW8u07wIjpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a pretrained embedding\n",
        "\n",
        "#Two options to obtain the embedding\n",
        "\n",
        "# Option 1: Direct download from Stanford\n",
        "\n",
        "# In this example we will adopt the GloVe with 50 dimensions: https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "#!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!unzip -q glove.6B.zip\n",
        "\n",
        "#!rm glove.6B.100d.txt\n",
        "#!rm glove.6B.200d.txt\n",
        "#!rm glove.6B.300d.txt\n",
        "#!rm glove.6B.zip\n",
        "\n",
        "\n",
        "# Option 2: Upload the file emb.zip to the working directory\n",
        "\n",
        "!unzip -q emb.zip\n",
        "\n",
        "!rm emb.zip"
      ],
      "metadata": {
        "id": "A9b-xGpHbCH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join('glove.6B.50d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "id": "0QohFC_zLdZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the embedding matrix with max_words lines and 50 columns (the embedding dimension)\n",
        "\n",
        "embedding_dim = 50\n",
        "max_words = max_features\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # matriz com zeros\n",
        "\n",
        "# Fill the matrix with the values of the pretrained embedding\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "59Rph_ncMBuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model D, which is identical to model C with the exception of the embedding dimension\n",
        "\n",
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "inputs = keras.Input(shape=[maxlen])\n",
        "emb = keras.layers.Embedding(max_words, embedding_dim)(inputs)\n",
        "\n",
        "### Complete the Missing layers ###\n",
        "\n",
        "output = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "modelD = keras.Model(inputs, output)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pGS6d-YDMEtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the pretrained embedding values in the embedding layer\n",
        "# Freeze the weights, so that they do not change during training\n",
        "\n",
        "modelD.layers[1].set_weights([embedding_matrix])\n",
        "modelD.layers[1].trainable = False"
      ],
      "metadata": {
        "id": "a3C8i89_e5Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelD.summary()"
      ],
      "metadata": {
        "id": "OZ--lQoCMLmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelD.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "history = modelD.fit(x_trainP, y_train,\n",
        "                      epochs=10,\n",
        "                      validation_split=0.2)"
      ],
      "metadata": {
        "id": "vjJUuaOyMZIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate modelD in the test set\n",
        "\n",
        "modelD.evaluate(x_testP, y_test)"
      ],
      "metadata": {
        "id": "jmWbSmoKMn1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Quiz**\n",
        "\n",
        "1. How do you evaluate the performance of Model D?\n",
        "\n",
        "2.Present justifications for the comparative accuracy of the different models."
      ],
      "metadata": {
        "id": "nQfPXKhkBEqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.\tText Preprocessing / Hyperparameter Analysis**\n",
        "\n",
        "Several preprocessing operations and hyperparameters are used in this example and they can have a relevant impact in the performance of the models. Create a new experiment by changing one of the following options and analyze the impact on performance.\n",
        "\n",
        "**3.1.\tReviews Preprocessing**\n",
        "\n",
        "a)\tDo not remove frequent words from the dataset.\n",
        "\n",
        "b)\tIs the relative order of the words relevant for classifying a review? (In this task, it might be useful to consult the permuted method from NumPy)\n",
        "\n",
        "c)\tChange the number of words kept in each review and check how it impacts results.\n",
        "\n",
        "d)\tEliminate the words at the end of the review and not at the beginning and check how it impacts results.\n",
        "\n",
        "e)\tTry varying the number of frequent words that are dropped and check how it impacts results.\n",
        "\n",
        "**3.2.\tHyperparameters**\n",
        "\n",
        "a)\tTest different values for hyperparameters (e.g., the embedding dimension) and check how it impacts results.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n-dhRLEjdfNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## CODE GOES HERE  ##\n"
      ],
      "metadata": {
        "id": "W9e1H7WcNeFI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
