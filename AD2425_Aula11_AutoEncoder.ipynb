{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
  {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FranciscoBPereira/AnaliseDados_2425_MEI_ISEC/blob/main/AD2425_Aula11_AutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup, Version check and Common imports\n",
        "\n",
        "# Python ≥3.8 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)\n",
        "\n",
        "print('Python version: ', sys.version_info)\n",
        "print('TF version: ', tf.__version__)\n",
        "print('Keras version: ', keras.__version__)\n",
        "print('GPU is', 'available' if tf.config.list_physical_devices('GPU') else 'NOT AVAILABLE')"
      ],
      "metadata": {
        "id": "7bxrhYHFhfit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Fetching and Preprocessing**"
      ],
      "metadata": {
        "id": "fI-kXbiPA0jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Fashion MNIST data from Keras datasets\n",
        "# In this task, targets are irrelevant. Therefore we just keep the 28*28 grayscale images\n",
        "\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "(x_train, _), (x_test, _) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "lx5l4dSA_c-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize images and adjust shape for autoencoder input\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "metadata": {
        "id": "dQ_DGJaG_qVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add noise to images.\n",
        "# The parameter noise_factor defines the strength of the perturbation\n",
        "# https://www.tensorflow.org/api_docs/python/tf/random/normal\n",
        "\n",
        "# After the noise perturbation, the values still belong to the interval [0, 1]\n",
        "\n",
        "noise_factor = 0.2\n",
        "\n",
        "x_train_noisy = x_train + noise_factor * tf.random.normal(shape=x_train.shape)\n",
        "x_test_noisy = x_test + noise_factor * tf.random.normal(shape=x_test.shape)\n",
        "\n",
        "x_train_noisy = tf.clip_by_value(x_train_noisy, clip_value_min=0., clip_value_max=1.)\n",
        "x_test_noisy = tf.clip_by_value(x_test_noisy, clip_value_min=0., clip_value_max=1.)"
      ],
      "metadata": {
        "id": "uzajUGwk_2MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize a few images before and after the perturbation\n",
        "# Change the value of start to inspect other images\n",
        "\n",
        "start= 0\n",
        "\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 6))\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.title(\"Originals\")\n",
        "    plt.imshow(tf.squeeze(x_test[i+start]))\n",
        "    plt.gray()\n",
        "\n",
        "    ax = plt.subplot(2, n, i + n + 1)\n",
        "    plt.title(\"Noisy\")\n",
        "    plt.imshow(tf.squeeze(x_test_noisy[i+start]))\n",
        "    plt.gray()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "06OkmF2E_8Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creation of the Denoising AutoEncoder**"
      ],
      "metadata": {
        "id": "bK39X1sSA4h_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Denoise is a subclass of the generic Model class\n",
        "# Complete the Decoder section. You should use Conv2DTranspose layers to reconstruct (upsample) the latent representation to the original image dimensions\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose\n",
        "# https://keras.io/api/models/model/#model-class\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "class Denoise(Model):\n",
        "  def __init__(self):\n",
        "    super(Denoise, self).__init__()\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      layers.Input(shape=(28, 28, 1)),\n",
        "      layers.Conv2D(16, (3, 3), activation='relu', padding='same', strides=2),\n",
        "      layers.Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)])\n",
        "\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      layers.Conv2DTranspose(8, (3,3), activation='relu', padding='same', strides=2),\n",
        "      layers.Conv2DTranspose(16, (3,3), activation='relu', padding='same', strides=2),\n",
        "      layers.Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same')])\n",
        "\n",
        "  def call(self, x):                    # When called with parameter x\n",
        "    encoded = self.encoder(x)           # The encoder creates a latent representation of x\n",
        "    decoded = self.decoder(encoded)     # Then, the decoder takes the latent representation and obtains x\n",
        "    return decoded"
      ],
      "metadata": {
        "id": "RRrKgU2NA8MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Autoencoder object\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "autoencoder = Denoise()"
      ],
      "metadata": {
        "id": "onTCgLxhBgqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compile and Train**"
      ],
      "metadata": {
        "id": "y9OHRgSBCTTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model compilation\n",
        "# This is a regression problem, where the loss corresponds to the differencde between the input (noisy image) and the output (original image)\n",
        "# Therefore, the Autoencoder must remove the noise from the input , aiming at delivering a clear image\n",
        "\n",
        "autoencoder.compile(optimizer='adam', loss=keras.losses.MeanSquaredError())"
      ],
      "metadata": {
        "id": "JP2T8E2NBmro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "autoencoder.fit(x_train_noisy, x_train,\n",
        "                epochs=10,\n",
        "                shuffle=True)"
      ],
      "metadata": {
        "id": "Z-G0my5qBwWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirm the reduction of dimension from the original image to the latent representation (Encoder)\n",
        "# Confirm the expansion from the latent representation to the original dimension (Decoder)\n",
        "\n",
        "autoencoder.summary()\n",
        "\n",
        "print('***Encoder***')\n",
        "autoencoder.encoder.summary()\n",
        "\n",
        "print('***Decoder***')\n",
        "autoencoder.decoder.summary()"
      ],
      "metadata": {
        "id": "6h2XxgkgB5T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate Performance**"
      ],
      "metadata": {
        "id": "iT7fs-TtCoAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate performance on the test set\n",
        "\n",
        "autoencoder.evaluate(x_test_noisy, x_test)"
      ],
      "metadata": {
        "id": "pj4x9ZmACW2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply trained autoencoder to images from the test set\n",
        "\n",
        "encoded_imgs = autoencoder.encoder(x_test_noisy).numpy()\n",
        "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()"
      ],
      "metadata": {
        "id": "xLQOuJnLCbkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize all transformations\n",
        "# 1. Original images\n",
        "# 2. Noise images\n",
        "# 3. Denoised images\n",
        "\n",
        "\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 10))\n",
        "for i in range(n):\n",
        "\n",
        "    # display original\n",
        "    ax = plt.subplot(3, n, i + 1)\n",
        "    plt.title(\"Originals\")\n",
        "    plt.imshow(tf.squeeze(x_test[i+start]))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display original + noise\n",
        "    ax = plt.subplot(3, n, i + n + 1)\n",
        "    plt.title(\"Noisy\")\n",
        "    plt.imshow(tf.squeeze(x_test_noisy[i+start]))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    bx = plt.subplot(3, n, i + n*2 + 1)\n",
        "    plt.title(\"Denoised\")\n",
        "    plt.imshow(tf.squeeze(decoded_imgs[i+start]))\n",
        "    plt.gray()\n",
        "    bx.get_xaxis().set_visible(False)\n",
        "    bx.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "otgi0DxACiGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**CIFAR10 GrayScale to RGB Autoencoder**"
      ],
      "metadata": {
        "id": "ve99CDghC_z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get CIFAR10 data from Keras datasets\n",
        "# In this task, targets are irrelevant. Therefore we just keep the 32*32 RGB images\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "(x_trainC, _), (x_testC, _) = cifar10.load_data()"
      ],
      "metadata": {
        "id": "7CLjdC31CswA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_trainC = x_trainC.astype('float32') / 255.\n",
        "x_testC = x_testC.astype('float32') / 255.\n",
        "\n",
        "print(x_trainC.shape)\n",
        "print(x_testC.shape)"
      ],
      "metadata": {
        "id": "IAJXu3YQDFLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Grayscale versions of the original images\n",
        "# Use method rgb_to_grayscale from TensorFlow: https://www.tensorflow.org/api_docs/python/tf/image/rgb_to_grayscale\n",
        "\n",
        "x_trainC_Gray = tf.image.rgb_to_grayscale(x_trainC)\n",
        "x_testC_Gray = tf.image.rgb_to_grayscale(x_testC)"
      ],
      "metadata": {
        "id": "4uG1WRjrkcYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_trainC_Gray.shape)\n",
        "print(x_testC_Gray.shape)"
      ],
      "metadata": {
        "id": "HF5X6hVLktFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz**\n",
        "\n",
        "The goal of the quiz is to create an AutoEncoder that is able to add color to a grayscale image.\n",
        "\n",
        "The dataset is already created, as we have RGB and Grayscale versions of the same images.\n",
        "\n",
        "Adapt the previous AutoEncoder so that is can be applied to this problem. In addition to the mandatory changes, the new AutoEncoder must have to additional CNN layers, both in the Encoder and in the Decoder.\n",
        "\n",
        "After creating the new AutoEncoder, compile, train and analyze results. Training should be longer, with a minimum of 30 epochs."
      ],
      "metadata": {
        "id": "cpuEAGjOZvnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## CODE GOES HERE ###\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kPgRBZXRow8I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
