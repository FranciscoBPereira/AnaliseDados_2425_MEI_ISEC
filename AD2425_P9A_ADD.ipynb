{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1CXm_kbADnl"
      },
      "outputs": [],
      "source": [
        "# Setup, Version check and Common imports\n",
        "\n",
        "# Python ≥3.8 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)\n",
        "\n",
        "print('Python version: ', sys.version_info)\n",
        "print('TF version: ', tf.__version__)\n",
        "print('Keras version: ', keras.__version__)\n",
        "print('GPU is', 'available' if tf.config.list_physical_devices('GPU') else 'NOT AVAILABLE')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0. Relevant Dataset Settings**"
      ],
      "metadata": {
        "id": "2o2_Th7lOZyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example from Keras tutorial: https://keras.io/examples/nlp/addition_rnn/\n",
        "\n",
        "# Dataset with TRAINING_SIZE examples\n",
        "TRAINING_SIZE = 20000\n",
        "\n",
        "# Maximum length of each operand is DIGITS.\n",
        "# Each example corresponds to a simple addition between two integer values, with digits between 1 and DIGITS\n",
        "DIGITS = 3\n",
        "\n",
        "# If REVERSE = True, then the input sequence is reversed\n",
        "REVERSE = False\n",
        "\n",
        "\n",
        "# Maximum length of input is 'int + int' (e.g., '345+678').\n",
        "MAXLEN = DIGITS + 1 + DIGITS\n",
        "\n",
        "# Maximum length of output is DIGITS + 1\n"
      ],
      "metadata": {
        "id": "GrputfIMV-DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Dataset Creation and Processing**"
      ],
      "metadata": {
        "id": "EVSTUBKZOyPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition of the class that will generate dataset examples\n",
        "\n",
        "# Characters are encoded as a one-hot integer representation\n",
        "\n",
        "class CharacterTable:\n",
        "    # Given a set of characters:\n",
        "    # Encode them to a one-hot integer representation\n",
        "    # Decode the one-hot or integer representation to their character output\n",
        "    # Decode a vector of probabilities to their character output\n",
        "\n",
        "    def __init__(self, chars):\n",
        "        # Initialize character table.\n",
        "        # Arguments\n",
        "        # chars: Characters that can appear in the input.\n",
        "\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "\n",
        "    def encode(self, C, num_rows):\n",
        "        # One-hot encode given string C.\n",
        "        # Arguments\n",
        "        # C: string, to be encoded.\n",
        "        # num_rows: Number of rows in the returned one-hot encoding. This is used to keep the # of rows for each data the same.\n",
        "\n",
        "        x = np.zeros((num_rows, len(self.chars)))\n",
        "        for i, c in enumerate(C):\n",
        "            x[i, self.char_indices[c]] = 1\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        # Decode the given vector or 2D array to their character output.\n",
        "        # Arguments\n",
        "        # x: A vector or a 2D array of probabilities or one-hot representations;\n",
        "        # or a vector of character indices (used with `calc_argmax=False`).\n",
        "        # calc_argmax: Whether to find the character index with maximum probability, defaults to `True`.\n",
        "\n",
        "        if calc_argmax:\n",
        "            x = x.argmax(axis=-1)\n",
        "        return \"\".join(self.indices_char[x] for x in x)\n",
        "\n",
        "\n",
        "# All the numbers, plus sign and space for padding.\n",
        "chars = \"0123456789+ \"\n",
        "ctable = CharacterTable(chars)\n",
        "\n",
        "questions = []\n",
        "expected = []\n",
        "seen = set()\n",
        "print(\"Generating data...\")\n",
        "while len(questions) < TRAINING_SIZE:\n",
        "    f = lambda: int(\n",
        "        \"\".join(\n",
        "            np.random.choice(list(\"0123456789\"))\n",
        "            for i in range(np.random.randint(1, DIGITS + 1))\n",
        "        )\n",
        "    )\n",
        "    a, b = f(), f()\n",
        "    # Skip any addition questions we've already seen\n",
        "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
        "    key = tuple(sorted((a, b)))\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    # Pad the data with spaces such that it is always MAXLEN.\n",
        "    q = \"{}+{}\".format(a, b)\n",
        "    query = q + \" \" * (MAXLEN - len(q))\n",
        "    ans = str(a + b)\n",
        "    # Answers can be of maximum size DIGITS + 1.\n",
        "    ans += \" \" * (DIGITS + 1 - len(ans))\n",
        "    if REVERSE:\n",
        "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
        "        # space used for padding.)\n",
        "        query = query[::-1]\n",
        "    questions.append(query)\n",
        "    expected.append(ans)\n",
        "print(\"Total questions:\", len(questions))\n"
      ],
      "metadata": {
        "id": "UoG02W5ShAgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Visualize a few examples\n",
        "\n",
        " data = np.random.randint(0, len(questions)-10)\n",
        "\n",
        " for i in range(10):\n",
        "   print(questions[data + i] + ' = ' + expected[data + i])"
      ],
      "metadata": {
        "id": "WRPswLbKhKJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data vectorization\n",
        "\n",
        "print(\"Vectorization...\")\n",
        "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=bool)\n",
        "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=bool)\n",
        "for i, sentence in enumerate(questions):\n",
        "    x[i] = ctable.encode(sentence, MAXLEN)\n",
        "for i, sentence in enumerate(expected):\n",
        "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
        "\n",
        "# Shuffle (x, y) in unison as the later parts of x will almost all be larger digits.\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "x = x[indices]\n",
        "y = y[indices]\n",
        "\n",
        "#Create train and validation datasets\n",
        "split_at = len(x) - len(x) // 10\n",
        "(x_train, x_val) = x[:split_at], x[split_at:]\n",
        "(y_train, y_val) = y[:split_at], y[split_at:]\n",
        "\n",
        "print(\"Training Data:\")\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(\"Validation Data:\")\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)\n"
      ],
      "metadata": {
        "id": "sq6gV588hZVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Deep Learning Model**"
      ],
      "metadata": {
        "id": "Qmki95cMO-Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "# See the worksheet for details\n",
        "\n",
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "inputs = keras.Input(shape=[MAXLEN, len(chars)])\n",
        "x = keras.layers.LSTM(128, return_sequences = False)(inputs)\n",
        "x = layers.RepeatVector(DIGITS + 1)(x)\n",
        "x = layers.LSTM(128, return_sequences=True)(x)\n",
        "output = layers.TimeDistributed(layers.Dense(len(chars), activation=\"softmax\"))(x)\n",
        "\n",
        "model = keras.Model(inputs, output)\n",
        "\n"
      ],
      "metadata": {
        "id": "V6Jg2ytzheWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Model Compilation, Training and Evaluation**"
      ],
      "metadata": {
        "id": "bf_ggWEXPPel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "mXvL7XPzhhoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "# In each epoch, a sample of 10 validation examples is presented\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "# Train the model each generation and show predictions against the validation dataset.\n",
        "for epoch in range(1, epochs):\n",
        "    print()\n",
        "    print(\"Iteration\", epoch)\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=1,\n",
        "        validation_data=(x_val, y_val),\n",
        "    )\n",
        "    # Select 10 samples from the validation set at random so we can visualize errors.\n",
        "    total = 0\n",
        "    error = 0\n",
        "    for i in range(10):\n",
        "        ind = np.random.randint(0, len(x_val))\n",
        "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "        preds = np.argmax(model.predict(rowx), axis=-1)\n",
        "        q = ctable.decode(rowx[0])\n",
        "        correct = ctable.decode(rowy[0])\n",
        "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
        "        print(\"Q\", q[::-1] if REVERSE else q, end=\" \")\n",
        "        print(\"T\", correct, end=\" \")\n",
        "        error += abs(int(correct)-int(guess))\n",
        "        if correct == guess:\n",
        "            total += 1\n",
        "            print(\"☑ \" + guess)\n",
        "        else:\n",
        "            print(\"☒ \" + guess)\n",
        "    print('Correct:', total/10, '(', total, '/10)')\n",
        "    print('MAE: ', error/10)\n",
        "\n"
      ],
      "metadata": {
        "id": "ieKJZ1aUhkak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate final performance on the validation set\n",
        "\n",
        "model.evaluate(x_val, y_val)\n"
      ],
      "metadata": {
        "id": "hVGRCOqVoWXE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}