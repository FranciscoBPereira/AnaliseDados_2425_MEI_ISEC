{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FranciscoBPereira/AnaliseDados_2425_MEI_ISEC/blob/main/AD2425_P9B_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhtrLE_6Qk5z"
      },
      "outputs": [],
      "source": [
        "# Setup, Version check and Common imports\n",
        "\n",
        "# Python ≥3.8 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)\n",
        "\n",
        "print('Python version: ', sys.version_info)\n",
        "print('TF version: ', tf.__version__)\n",
        "print('Keras version: ', keras.__version__)\n",
        "print('GPU is', 'available' if tf.config.list_physical_devices('GPU') else 'NOT AVAILABLE')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_2v2dj0mm0m"
      },
      "source": [
        "**1. Dataset Loading, Preprocessing and Vectorization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VFgUHefXHCs"
      },
      "source": [
        "**1.1. Loading, Slicing and Dividing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8OX4aRnRU0C"
      },
      "outputs": [],
      "source": [
        "# Upload file \"por.txt\" to the working directory\n",
        "# If you place the file in another directory, adjust the path in first line of code\n",
        "# Add \"start\" and \"end\" to target strings\n",
        "\n",
        "text_file = 'por.txt'\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "or_text_pairs = []\n",
        "for line in lines:\n",
        "    english, port = line.split(\"\\t\")\n",
        "    port = \"[start] \" + port + \" [end]\"\n",
        "    or_text_pairs.append((english, port))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uULMxaI3Rgll"
      },
      "outputs": [],
      "source": [
        "# Visualize a random sample of 10 examples\n",
        "\n",
        "\n",
        "import random\n",
        "for x in range(10):\n",
        "  print(x+1 , \": \", random.choice(or_text_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV2zIumaaADe"
      },
      "outputs": [],
      "source": [
        "# Keep just 15% of the original dataset (for efficiency reasons). This will hurt performance\n",
        "# Split the remaining dataset in training (80%) and validation (20%)\n",
        "\n",
        "import random\n",
        "random.shuffle(or_text_pairs)\n",
        "\n",
        "slice = int(len(or_text_pairs)*0.15)\n",
        "\n",
        "text_pairs = or_text_pairs[ : slice]\n",
        "\n",
        "num_val_samples = int(0.20 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - num_val_samples\n",
        "\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4TYLI9RaAzN"
      },
      "outputs": [],
      "source": [
        "# Check dimensions\n",
        "\n",
        "print('Original: ', len(or_text_pairs), '\\n')\n",
        "\n",
        "print('Sliced: ', len(text_pairs))\n",
        "\n",
        "print('Training: ', len(train_pairs))\n",
        "\n",
        "print('Validation: ', len(val_pairs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPwvGXeCYdA5"
      },
      "source": [
        "**1.2. Preprocessing and Vectorization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KABs4N-Iae5j"
      },
      "outputs": [],
      "source": [
        "# Text vectorization\n",
        "# There is a method for standardization of the Portuguese text, in order to keep the special symbols [ ]\n",
        "\n",
        "import string\n",
        "import re\n",
        "\n",
        "strip_chars = string.punctuation\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "# Vectorization parameters\n",
        "sequence_length = 20\n",
        "vocab_size = 5000\n",
        "\n",
        "# Vectorization layer for English\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens = vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# Vectorization layer for Portuguese\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens = vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "# Get the vocabulary for each language\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_pt_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_pt_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tc1VtJqEawnP"
      },
      "outputs": [],
      "source": [
        "# English vocabulary\n",
        "\n",
        "source_vectorization.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUo1V-xpazXK"
      },
      "outputs": [],
      "source": [
        "# Portuguese vocabulary\n",
        "\n",
        "target_vectorization.get_vocabulary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmhJA6tHbdu3"
      },
      "source": [
        "**1.3. Pipelining**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhaGC8HBa2cs"
      },
      "outputs": [],
      "source": [
        "# Create Dataset objects for training\n",
        "# https://www.tensorflow.org/api_docs/python/tf/data\n",
        "\n",
        "# Objects are tuples (inputs, target)\n",
        "# inputs: dictionary with 2 keys \"encoder_inputs\" and \"decoder_inputs\", corresponding to the English and Portuguese sentences\n",
        "# target: Portuguese sentence, with the offset of one position to the right\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "def format_dataset(eng, pt):\n",
        "    eng = source_vectorization(eng)\n",
        "    pt = target_vectorization(pt)\n",
        "    return ({\n",
        "        \"english\": eng,\n",
        "        \"portuguese\": pt[:, :-1],\n",
        "    }, pt[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, pt_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    pt_texts = list(pt_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, pt_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSRLn1eIa-0P"
      },
      "outputs": [],
      "source": [
        "# Visualize the format\n",
        "\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['portuguese'].shape: {inputs['portuguese'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")\n",
        "    print(inputs['english'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjBl97uuzosj"
      },
      "source": [
        "Quiz:\n",
        "\n",
        "Identify all preprocessing operations that are performed to the text, since it is loaded from the file until it is ready to enter the neural network.\n",
        "For each case, explain with a simple sentence the goal of the operation.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec50HjKSgH6T"
      },
      "source": [
        "**2. The Seq2Seq Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xg5AcFHgbkNt"
      },
      "outputs": [],
      "source": [
        "# GRU-based encoder\n",
        "# One bidirectional layer of GRU cells\n",
        "# The layer output is sent to the decoder\n",
        "\n",
        "embed_dim = 256\n",
        "latent_dim = 1024\n",
        "\n",
        "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
        "encoded_source = layers.Bidirectional(\n",
        "    layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKi3Qn1Gbvo7"
      },
      "outputs": [],
      "source": [
        "# GRU-based decoder\n",
        "# One directional layer of GRU cells\n",
        "# It has a final Dense layer with dimension of the vocabulary and with softmax activation function. It predicts the next word.\n",
        "\n",
        "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"portuguese\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
        "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
        "x = decoder_gru(x, initial_state=encoded_source)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "# Full encoder-decoder model\n",
        "seq2seq_rnn = keras.Model([source, past_target], target_next_step)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTLLyxAaf7lj"
      },
      "outputs": [],
      "source": [
        "seq2seq_rnn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeOydaI90zcZ"
      },
      "source": [
        "Quiz:\n",
        "\n",
        "1. In the encoder the GRU layer is bidirectional, whereas in the decoder it is a standard unidirectional recurrent layer. Explain the advantages/constraints that justify this difference.\n",
        "\n",
        "2. Specify:\n",
        "\n",
        "  a) The dimensions of the embeddings in the encoder and in the decoder\n",
        "\n",
        "  b) The number of GRU cells in the encoder and in the decoder\n",
        "\n",
        "  c) The dimensions of the tensor that contains the information passed from the encoder to the decoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ibi90LVdb9qd"
      },
      "outputs": [],
      "source": [
        "# Compile and train\n",
        "\n",
        "seq2seq_rnn.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YficlsstlMTj"
      },
      "source": [
        "**3. Translating new Sentences**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hh9lgoVg_aL"
      },
      "outputs": [],
      "source": [
        "# Try translate a few sentences from English to Portuguese\n",
        "\n",
        "pt_vocab = target_vectorization.get_vocabulary()\n",
        "pt_index_lookup = dict(zip(range(len(pt_vocab)), pt_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
        "        next_token_predictions = seq2seq_rnn.predict(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
        "        sampled_token = pt_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "for _ in range(5):\n",
        "    input_sentence = input()\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence), '\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
